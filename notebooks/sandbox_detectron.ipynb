{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.data import build_detection_train_loader, build_detection_test_loader\n",
    "\n",
    "from resnet import ResNet\n",
    "from super_pycocotools.coco import SuperCOCO"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing DeepFruits\n",
      "train data\n",
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "super_ann_file = './datasets.json'\n",
    "coco = SuperCOCO(super_ann_file)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "malata-1_49.jpg (481, 360) (519, 389)\n",
      "malata-1_49.jpg (481, 360) (519, 389)\n",
      "Esselte_53.jpg (496, 378) (495, 394)\n",
      "McDonald's_64.jpg (491, 375) (501, 393)\n",
      "McDonald's_71.jpg (480, 361) (513, 388)\n",
      "McDonald's_79.jpg (503, 399) (359, 519)\n",
      "McDonald's_0.jpg (489, 355) (517, 392)\n",
      "McDonald's_24.jpg (509, 386) (358, 487)\n",
      "McDonald's_18.jpg (497, 380) (503, 381)\n",
      "McDonald's_12.jpg (522, 397) (497, 390)\n",
      "McDonald's_5.jpg (512, 354) (512, 366)\n",
      "McDonald's_83.jpg (520, 382) (352, 523)\n",
      "McDonald's_55.jpg (521, 362) (496, 353)\n",
      "McDonald's_41.jpg (481, 392) (500, 380)\n",
      "McDonald's_46.jpg (485, 361) (493, 386)\n",
      "McDonald's_89.jpg (491, 399) (482, 356)\n",
      "McDonald's_93.jpg (512, 394) (366, 485)\n",
      "McDonald's_14.jpg (493, 365) (492, 391)\n",
      "McDonald's_61.jpg (512, 392) (509, 399)\n",
      "McDonald's_90.jpg (487, 358) (362, 514)\n",
      "McDonald's_39.jpg (519, 397) (377, 516)\n",
      "McDonald's_28.jpg (495, 375) (379, 488)\n",
      "McDonald's_16.jpg (481, 360) (484, 357)\n",
      "McDonald's_27.jpg (503, 350) (510, 384)\n",
      "McDonald's_47.jpg (521, 392) (505, 373)\n",
      "McDonald's_95.jpg (487, 377) (487, 360)\n",
      "McDonald's_88.jpg (500, 350) (500, 352)\n",
      "McDonald's_67.jpg (482, 361) (508, 368)\n",
      "McDonald's_72.jpg (513, 398) (515, 374)\n",
      "McDonald's_94.jpg (391, 514) (492, 383)\n",
      "McDonald's_91.jpg (484, 362) (396, 519)\n",
      "McDonald's_92.jpg (500, 379) (492, 350)\n",
      "McDonald's_21.jpg (523, 386) (498, 376)\n",
      "McDonald's_51.jpg (487, 377) (480, 373)\n",
      "McDonald's_38.jpg (512, 388) (488, 388)\n",
      "McDonald's_1.jpg (506, 388) (485, 393)\n",
      "McDonald's_60.jpg (512, 384) (515, 365)\n",
      "McDonald's_48.jpg (516, 377) (487, 387)\n",
      "McDonald's_80.jpg (513, 374) (496, 397)\n",
      "McDonald's_3.jpg (488, 399) (511, 388)\n",
      "McDonald's_19.jpg (514, 396) (482, 356)\n",
      "McDonald's_73.jpg (492, 377) (492, 360)\n",
      "McDonald's_6.jpg (375, 494) (518, 377)\n",
      "McDonald's_86.jpg (487, 363) (393, 511)\n",
      "McDonald's_20.jpg (503, 377) (364, 487)\n",
      "McDonald's_35.jpg (488, 373) (489, 376)\n",
      "McDonald's_57.jpg (519, 398) (494, 379)\n",
      "McDonald's_70.jpg (519, 381) (502, 358)\n",
      "McDonald's_50.jpg (523, 382) (369, 481)\n",
      "McDonald's_43.jpg (481, 377) (481, 388)\n",
      "McDonald's_8.jpg (522, 373) (493, 357)\n",
      "McDonald's_2.jpg (493, 366) (389, 505)\n",
      "McDonald's_53.jpg (524, 354) (524, 366)\n",
      "McDonald's_69.jpg (497, 357) (504, 389)\n",
      "McDonald's_59.jpg (503, 380) (511, 390)\n",
      "McDonald's_29.jpg (492, 378) (354, 497)\n",
      "McDonald's_45.jpg (524, 369) (507, 392)\n",
      "McDonald's_30.jpg (505, 380) (499, 355)\n",
      "McDonald's_11.jpg (496, 377) (490, 369)\n",
      "McDonald's_31.jpg (488, 375) (488, 365)\n",
      "McDonald's_37.jpg (368, 480) (396, 487)\n",
      "McDonald's_62.jpg (505, 366) (523, 351)\n",
      "McDonald's_52.jpg (395, 522) (501, 351)\n",
      "McDonald's_32.jpg (488, 386) (524, 363)\n",
      "McDonald's_84.jpg (517, 370) (496, 398)\n",
      "McDonald's_36.jpg (494, 398) (495, 380)\n",
      "McDonald's_23.jpg (524, 370) (483, 380)\n",
      "toasties_35.jpg (522, 393) (389, 487)\n",
      "Compact_75.jpg (491, 374) (489, 377)\n",
      "Compact_0.jpg (491, 383) (512, 390)\n",
      "Compact_18.jpg (493, 378) (484, 381)\n",
      "Compact_12.jpg (509, 397) (520, 387)\n",
      "Compact_5.jpg (513, 398) (508, 357)\n",
      "Compact_14.jpg (504, 388) (387, 500)\n",
      "Compact_28.jpg (513, 381) (524, 353)\n",
      "Compact_72.jpg (516, 358) (481, 370)\n",
      "Compact_7.jpg (487, 368) (491, 383)\n",
      "Compact_20.jpg (511, 370) (497, 367)\n",
      "Compact_33.jpg (524, 379) (518, 367)\n",
      "Compact_23.jpg (488, 350) (480, 385)\n",
      "Indomie Mi goreng_8.jpg (523, 371) (484, 398)\n",
      "Honey Nut Corn Flakes_17.jpg (367, 511) (486, 389)\n",
      "Honey Nut Corn Flakes_26.jpg (374, 524) (357, 514)\n",
      "Honey Nut Corn Flakes_64.jpg (515, 371) (384, 518)\n",
      "Honey Nut Corn Flakes_58.jpg (517, 370) (391, 524)\n",
      "Honey Nut Corn Flakes_0.jpg (482, 380) (484, 352)\n",
      "Honey Nut Corn Flakes_24.jpg (514, 375) (507, 391)\n",
      "Honey Nut Corn Flakes_18.jpg (386, 510) (513, 378)\n",
      "Honey Nut Corn Flakes_12.jpg (495, 374) (487, 355)\n",
      "Honey Nut Corn Flakes_5.jpg (365, 508) (352, 517)\n",
      "Honey Nut Corn Flakes_55.jpg (480, 388) (520, 350)\n",
      "Honey Nut Corn Flakes_41.jpg (385, 514) (480, 350)\n",
      "Honey Nut Corn Flakes_46.jpg (510, 363) (393, 499)\n",
      "Honey Nut Corn Flakes_49.jpg (490, 358) (488, 398)\n",
      "Honey Nut Corn Flakes_14.jpg (519, 352) (378, 489)\n",
      "Honey Nut Corn Flakes_42.jpg (393, 507) (383, 517)\n",
      "Honey Nut Corn Flakes_61.jpg (521, 354) (508, 381)\n",
      "Honey Nut Corn Flakes_39.jpg (384, 485) (384, 497)\n",
      "Honey Nut Corn Flakes_10.jpg (357, 508) (366, 517)\n",
      "Honey Nut Corn Flakes_28.jpg (516, 390) (524, 384)\n",
      "Honey Nut Corn Flakes_16.jpg (356, 482) (516, 392)\n",
      "Honey Nut Corn Flakes_47.jpg (365, 514) (386, 522)\n",
      "Honey Nut Corn Flakes_7.jpg (508, 362) (508, 380)\n",
      "Honey Nut Corn Flakes_13.jpg (513, 386) (350, 520)\n",
      "Honey Nut Corn Flakes_21.jpg (365, 507) (386, 516)\n",
      "Honey Nut Corn Flakes_51.jpg (389, 523) (365, 514)\n",
      "Honey Nut Corn Flakes_63.jpg (487, 356) (516, 381)\n",
      "Honey Nut Corn Flakes_38.jpg (385, 515) (398, 481)\n",
      "Honey Nut Corn Flakes_44.jpg (399, 507) (480, 373)\n",
      "Honey Nut Corn Flakes_1.jpg (517, 358) (519, 394)\n",
      "Honey Nut Corn Flakes_60.jpg (360, 512) (388, 508)\n",
      "Honey Nut Corn Flakes_48.jpg (382, 482) (495, 399)\n",
      "Honey Nut Corn Flakes_3.jpg (373, 503) (501, 365)\n",
      "Honey Nut Corn Flakes_19.jpg (496, 392) (359, 486)\n",
      "Honey Nut Corn Flakes_6.jpg (392, 511) (493, 380)\n",
      "Honey Nut Corn Flakes_20.jpg (389, 480) (353, 504)\n",
      "Honey Nut Corn Flakes_35.jpg (391, 522) (397, 506)\n",
      "Honey Nut Corn Flakes_57.jpg (394, 512) (510, 396)\n",
      "Honey Nut Corn Flakes_54.jpg (372, 524) (386, 499)\n",
      "Honey Nut Corn Flakes_15.jpg (350, 518) (351, 516)\n",
      "Honey Nut Corn Flakes_50.jpg (503, 362) (517, 395)\n",
      "Honey Nut Corn Flakes_43.jpg (508, 379) (399, 494)\n",
      "Honey Nut Corn Flakes_33.jpg (503, 357) (504, 350)\n",
      "Honey Nut Corn Flakes_8.jpg (362, 489) (516, 360)\n",
      "Honey Nut Corn Flakes_2.jpg (483, 391) (354, 506)\n",
      "Honey Nut Corn Flakes_53.jpg (508, 351) (509, 369)\n",
      "Honey Nut Corn Flakes_40.jpg (515, 378) (482, 389)\n",
      "Honey Nut Corn Flakes_22.jpg (395, 490) (363, 496)\n",
      "Honey Nut Corn Flakes_34.jpg (386, 484) (370, 480)\n",
      "Honey Nut Corn Flakes_59.jpg (398, 522) (390, 502)\n",
      "Honey Nut Corn Flakes_29.jpg (491, 388) (361, 485)\n",
      "Honey Nut Corn Flakes_45.jpg (382, 487) (522, 390)\n",
      "Honey Nut Corn Flakes_30.jpg (378, 485) (482, 377)\n",
      "Honey Nut Corn Flakes_11.jpg (501, 379) (372, 490)\n",
      "Honey Nut Corn Flakes_31.jpg (397, 490) (504, 395)\n",
      "Honey Nut Corn Flakes_37.jpg (493, 360) (489, 351)\n",
      "Honey Nut Corn Flakes_62.jpg (511, 365) (493, 362)\n",
      "Honey Nut Corn Flakes_32.jpg (524, 392) (516, 359)\n",
      "Honey Nut Corn Flakes_36.jpg (506, 350) (388, 480)\n",
      "Honey Nut Corn Flakes_9.jpg (378, 520) (367, 496)\n",
      "Honey Nut Corn Flakes_4.jpg (359, 491) (375, 489)\n",
      "Honey Nut Corn Flakes_23.jpg (361, 502) (501, 369)\n",
      "Honey Nut Corn Flakes_56.jpg (489, 397) (500, 371)\n",
      "Cream of Wheat_1.jpg (522, 372) (490, 389)\n",
      "Cream of Wheat_2.jpg (481, 390) (356, 518)\n",
      "Cocoa Krispies_26.jpg (494, 368) (370, 502)\n",
      "Cocoa Krispies_24.jpg (382, 503) (353, 494)\n",
      "Cocoa Krispies_18.jpg (524, 397) (381, 511)\n",
      "Cocoa Krispies_12.jpg (517, 388) (359, 504)\n",
      "Cocoa Krispies_5.jpg (496, 386) (359, 515)\n",
      "Cocoa Krispies_42.jpg (519, 368) (491, 357)\n",
      "Cocoa Krispies_28.jpg (388, 485) (502, 372)\n",
      "Cocoa Krispies_27.jpg (381, 501) (503, 386)\n",
      "Cocoa Krispies_21.jpg (380, 491) (516, 375)\n",
      "Cocoa Krispies_51.jpg (480, 382) (359, 499)\n",
      "Cocoa Krispies_60.jpg (391, 488) (392, 505)\n",
      "Cocoa Krispies_35.jpg (505, 356) (375, 513)\n",
      "Cocoa Krispies_50.jpg (493, 374) (397, 503)\n",
      "Cocoa Krispies_53.jpg (506, 366) (389, 524)\n",
      "Cocoa Krispies_31.jpg (394, 506) (489, 356)\n",
      "Amora_0.jpg (480, 375) (518, 372)\n",
      "INOHERB-2_51.jpg (390, 485) (522, 364)\n",
      "INOHERB-2_35.jpg (392, 514) (524, 394)\n",
      "iPhone-1_110.jpg (375, 512) (484, 390)\n",
      "INOHERB-2_51.jpg (390, 485) (522, 364)\n",
      "INOHERB-2_35.jpg (392, 514) (524, 394)\n",
      "iPhone-1_110.jpg (375, 512) (484, 390)\n",
      "Harvest Moon_12.jpg (355, 483) (508, 360)\n",
      "liu gong_58.jpg (490, 362) (502, 386)\n",
      "liu gong_24.jpg (495, 371) (501, 399)\n",
      "liu gong_5.jpg (481, 359) (497, 380)\n",
      "liu gong_83.jpg (511, 376) (502, 359)\n",
      "liu gong_85.jpg (369, 496) (494, 382)\n",
      "liu gong_39.jpg (504, 380) (488, 375)\n",
      "liu gong_67.jpg (509, 384) (503, 375)\n",
      "liu gong_1.jpg (523, 358) (480, 398)\n",
      "liu gong_87.jpg (506, 362) (497, 378)\n",
      "liu gong_3.jpg (487, 387) (486, 353)\n",
      "liu gong_20.jpg (519, 383) (518, 350)\n",
      "liu gong_35.jpg (507, 395) (518, 398)\n",
      "liu gong_81.jpg (495, 353) (499, 366)\n",
      "liu gong_96.jpg (501, 358) (489, 361)\n",
      "liu gong_8.jpg (493, 398) (481, 357)\n",
      "liu gong_69.jpg (492, 361) (503, 398)\n",
      "liu gong_78.jpg (486, 391) (506, 392)\n",
      "liu gong_11.jpg (509, 389) (522, 398)\n",
      "liu gong_65.jpg (502, 360) (496, 390)\n",
      "liu gong_23.jpg (502, 372) (500, 387)\n",
      "snow peak-2_36.jpg (488, 385) (379, 507)\n",
      "snow peak-2_36.jpg (488, 385) (379, 507)\n"
     ]
    }
   ],
   "source": [
    "wrong_ann = []\n",
    "for id in coco.LogoDet.train.getImgIds():\n",
    "    im = coco.LogoDet.train.loadImgs(id)[0]\n",
    "    image = Image.open('/home/cose-ia/Data/LogoDet/train2017/' + im['file_name'])\n",
    "\n",
    "    if image.size != (im['width'], im['height']):\n",
    "        print(im['file_name'], image.size, (im['width'], im['height']))\n",
    "        wrong_ann.append((im['file_name'], image.size, (im['width'], im['height'])))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "190"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wrong_ann)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lettres ['a', 'b', 'c']\n",
      "chiffres [1, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "dict = {\n",
    "    'lettres': ['a', 'b', 'c'],\n",
    "    'chiffres': [1, 2, 3],\n",
    "}\n",
    "for lettre, chiffre in dict.items():\n",
    "    print(lettre, chiffre)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "for a in [None]:\n",
    "    print(a is None)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "dict_items([('lettres', ['a', 'b', 'c']), ('chiffres', [1, 2, 3]), ('seed', 42)])"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict.items()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'coco' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[5], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m data_dir \u001B[38;5;241m=\u001B[39m \u001B[43mcoco\u001B[49m\u001B[38;5;241m.\u001B[39mDeepFruits\u001B[38;5;241m.\u001B[39minfos[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mroot\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[1;32m      2\u001B[0m data_type \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtrain2017\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m      3\u001B[0m img_dir \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m/\u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m/\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mformat(data_dir, data_type)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'coco' is not defined"
     ]
    }
   ],
   "source": [
    "data_dir = coco.DeepFruits.infos['root']\n",
    "data_type = 'train2017'\n",
    "img_dir = '{}/{}/'.format(data_dir, data_type)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'coco' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[3], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m cfg \u001B[38;5;241m=\u001B[39m get_cfg()\n\u001B[0;32m----> 2\u001B[0m cfg\u001B[38;5;241m.\u001B[39mDATASETS\u001B[38;5;241m.\u001B[39mTRAIN \u001B[38;5;241m=\u001B[39m \u001B[43mcoco\u001B[49m\u001B[38;5;241m.\u001B[39mDeepFruits\u001B[38;5;241m.\u001B[39mregister()[\u001B[38;5;241m0\u001B[39m]\n",
      "\u001B[0;31mNameError\u001B[0m: name 'coco' is not defined"
     ]
    }
   ],
   "source": [
    "cfg = get_cfg()\n",
    "cfg.DATASETS.TRAIN = coco.DeepFruits.register()[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "dict_keys(['VERSION', 'MODEL', 'INPUT', 'DATASETS', 'DATALOADER', 'SOLVER', 'TEST', 'OUTPUT_DIR', 'SEED', 'CUDNN_BENCHMARK', 'VIS_PERIOD', 'GLOBAL'])"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg.keys()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "data": {
      "text/plain": "457"
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataloader.dataset.dataset.dataset)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "cfg.SOLVER.IMS_PER_BATCH = 1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Dataset 'DeepFruits_train' is not registered! Available datasets are: coco_2014_train, coco_2014_val, coco_2014_minival, coco_2014_valminusminival, coco_2017_train, coco_2017_val, coco_2017_test, coco_2017_test-dev, coco_2017_val_100, keypoints_coco_2014_train, keypoints_coco_2014_val, keypoints_coco_2014_minival, keypoints_coco_2014_valminusminival, keypoints_coco_2017_train, keypoints_coco_2017_val, keypoints_coco_2017_val_100, coco_2017_train_panoptic_separated, coco_2017_train_panoptic_stuffonly, coco_2017_train_panoptic, coco_2017_val_panoptic_separated, coco_2017_val_panoptic_stuffonly, coco_2017_val_panoptic, coco_2017_val_100_panoptic_separated, coco_2017_val_100_panoptic_stuffonly, coco_2017_val_100_panoptic, lvis_v1_train, lvis_v1_val, lvis_v1_test_dev, lvis_v1_test_challenge, lvis_v0.5_train, lvis_v0.5_val, lvis_v0.5_val_rand_100, lvis_v0.5_test, lvis_v0.5_train_cocofied, lvis_v0.5_val_cocofied, cityscapes_fine_instance_seg_train, cityscapes_fine_sem_seg_train, cityscapes_fine_instance_seg_val, cityscapes_fine_sem_seg_val, cityscapes_fine_instance_seg_test, cityscapes_fine_sem_seg_test, cityscapes_fine_panoptic_train, cityscapes_fine_panoptic_val, voc_2007_trainval, voc_2007_train, voc_2007_val, voc_2007_test, voc_2012_trainval, voc_2012_train, voc_2012_val, ade20k_sem_seg_train, ade20k_sem_seg_val\"",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "File \u001B[0;32m~/detectron2/detectron2/data/catalog.py:51\u001B[0m, in \u001B[0;36m_DatasetCatalog.get\u001B[0;34m(self, name)\u001B[0m\n\u001B[1;32m     50\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 51\u001B[0m     f \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m[\u001B[49m\u001B[43mname\u001B[49m\u001B[43m]\u001B[49m\n\u001B[1;32m     52\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "File \u001B[0;32m~/miniconda3/envs/detr/lib/python3.10/collections/__init__.py:1106\u001B[0m, in \u001B[0;36mUserDict.__getitem__\u001B[0;34m(self, key)\u001B[0m\n\u001B[1;32m   1105\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__missing__\u001B[39m(\u001B[38;5;28mself\u001B[39m, key)\n\u001B[0;32m-> 1106\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(key)\n",
      "\u001B[0;31mKeyError\u001B[0m: 'DeepFruits_train'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[9], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m dataloader \u001B[38;5;241m=\u001B[39m \u001B[43mbuild_detection_test_loader\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcfg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mDeepFruits_train\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/detectron2/detectron2/config/config.py:207\u001B[0m, in \u001B[0;36mconfigurable.<locals>.wrapper.<locals>.wrapped\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    204\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(orig_func)\n\u001B[1;32m    205\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mwrapped\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m    206\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m _called_with_cfg(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m--> 207\u001B[0m         explicit_args \u001B[38;5;241m=\u001B[39m \u001B[43m_get_args_from_config\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfrom_config\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    208\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m orig_func(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mexplicit_args)\n\u001B[1;32m    209\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[0;32m~/detectron2/detectron2/config/config.py:245\u001B[0m, in \u001B[0;36m_get_args_from_config\u001B[0;34m(from_config_func, *args, **kwargs)\u001B[0m\n\u001B[1;32m    243\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m supported_arg_names:\n\u001B[1;32m    244\u001B[0m         extra_kwargs[name] \u001B[38;5;241m=\u001B[39m kwargs\u001B[38;5;241m.\u001B[39mpop(name)\n\u001B[0;32m--> 245\u001B[0m ret \u001B[38;5;241m=\u001B[39m \u001B[43mfrom_config_func\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    246\u001B[0m \u001B[38;5;66;03m# forward the other arguments to __init__\u001B[39;00m\n\u001B[1;32m    247\u001B[0m ret\u001B[38;5;241m.\u001B[39mupdate(extra_kwargs)\n",
      "File \u001B[0;32m~/detectron2/detectron2/data/build.py:461\u001B[0m, in \u001B[0;36m_test_loader_from_config\u001B[0;34m(cfg, dataset_name, mapper)\u001B[0m\n\u001B[1;32m    458\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(dataset_name, \u001B[38;5;28mstr\u001B[39m):\n\u001B[1;32m    459\u001B[0m     dataset_name \u001B[38;5;241m=\u001B[39m [dataset_name]\n\u001B[0;32m--> 461\u001B[0m dataset \u001B[38;5;241m=\u001B[39m \u001B[43mget_detection_dataset_dicts\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    462\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdataset_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    463\u001B[0m \u001B[43m    \u001B[49m\u001B[43mfilter_empty\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    464\u001B[0m \u001B[43m    \u001B[49m\u001B[43mproposal_files\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\n\u001B[1;32m    465\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcfg\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mDATASETS\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mPROPOSAL_FILES_TEST\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;28;43mlist\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mcfg\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mDATASETS\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mTEST\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mindex\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\u001B[43m]\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mx\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mdataset_name\u001B[49m\n\u001B[1;32m    466\u001B[0m \u001B[43m    \u001B[49m\u001B[43m]\u001B[49m\n\u001B[1;32m    467\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mcfg\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mMODEL\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mLOAD_PROPOSALS\u001B[49m\n\u001B[1;32m    468\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    469\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    470\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m mapper \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    471\u001B[0m     mapper \u001B[38;5;241m=\u001B[39m DatasetMapper(cfg, \u001B[38;5;28;01mFalse\u001B[39;00m)\n",
      "File \u001B[0;32m~/detectron2/detectron2/data/build.py:241\u001B[0m, in \u001B[0;36mget_detection_dataset_dicts\u001B[0;34m(names, filter_empty, min_keypoints, proposal_files, check_consistency)\u001B[0m\n\u001B[1;32m    239\u001B[0m     names \u001B[38;5;241m=\u001B[39m [names]\n\u001B[1;32m    240\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(names), names\n\u001B[0;32m--> 241\u001B[0m dataset_dicts \u001B[38;5;241m=\u001B[39m [DatasetCatalog\u001B[38;5;241m.\u001B[39mget(dataset_name) \u001B[38;5;28;01mfor\u001B[39;00m dataset_name \u001B[38;5;129;01min\u001B[39;00m names]\n\u001B[1;32m    243\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(dataset_dicts[\u001B[38;5;241m0\u001B[39m], torchdata\u001B[38;5;241m.\u001B[39mDataset):\n\u001B[1;32m    244\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(dataset_dicts) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m    245\u001B[0m         \u001B[38;5;66;03m# ConcatDataset does not work for iterable style dataset.\u001B[39;00m\n\u001B[1;32m    246\u001B[0m         \u001B[38;5;66;03m# We could support concat for iterable as well, but it's often\u001B[39;00m\n\u001B[1;32m    247\u001B[0m         \u001B[38;5;66;03m# not a good idea to concat iterables anyway.\u001B[39;00m\n",
      "File \u001B[0;32m~/detectron2/detectron2/data/build.py:241\u001B[0m, in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m    239\u001B[0m     names \u001B[38;5;241m=\u001B[39m [names]\n\u001B[1;32m    240\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(names), names\n\u001B[0;32m--> 241\u001B[0m dataset_dicts \u001B[38;5;241m=\u001B[39m [\u001B[43mDatasetCatalog\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataset_name\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m dataset_name \u001B[38;5;129;01min\u001B[39;00m names]\n\u001B[1;32m    243\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(dataset_dicts[\u001B[38;5;241m0\u001B[39m], torchdata\u001B[38;5;241m.\u001B[39mDataset):\n\u001B[1;32m    244\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(dataset_dicts) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m    245\u001B[0m         \u001B[38;5;66;03m# ConcatDataset does not work for iterable style dataset.\u001B[39;00m\n\u001B[1;32m    246\u001B[0m         \u001B[38;5;66;03m# We could support concat for iterable as well, but it's often\u001B[39;00m\n\u001B[1;32m    247\u001B[0m         \u001B[38;5;66;03m# not a good idea to concat iterables anyway.\u001B[39;00m\n",
      "File \u001B[0;32m~/detectron2/detectron2/data/catalog.py:53\u001B[0m, in \u001B[0;36m_DatasetCatalog.get\u001B[0;34m(self, name)\u001B[0m\n\u001B[1;32m     51\u001B[0m     f \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m[name]\n\u001B[1;32m     52\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m---> 53\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(\n\u001B[1;32m     54\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataset \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m is not registered! Available datasets are: \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\n\u001B[1;32m     55\u001B[0m             name, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(\u001B[38;5;28mlist\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mkeys()))\n\u001B[1;32m     56\u001B[0m         )\n\u001B[1;32m     57\u001B[0m     ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01me\u001B[39;00m\n\u001B[1;32m     58\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m f()\n",
      "\u001B[0;31mKeyError\u001B[0m: \"Dataset 'DeepFruits_train' is not registered! Available datasets are: coco_2014_train, coco_2014_val, coco_2014_minival, coco_2014_valminusminival, coco_2017_train, coco_2017_val, coco_2017_test, coco_2017_test-dev, coco_2017_val_100, keypoints_coco_2014_train, keypoints_coco_2014_val, keypoints_coco_2014_minival, keypoints_coco_2014_valminusminival, keypoints_coco_2017_train, keypoints_coco_2017_val, keypoints_coco_2017_val_100, coco_2017_train_panoptic_separated, coco_2017_train_panoptic_stuffonly, coco_2017_train_panoptic, coco_2017_val_panoptic_separated, coco_2017_val_panoptic_stuffonly, coco_2017_val_panoptic, coco_2017_val_100_panoptic_separated, coco_2017_val_100_panoptic_stuffonly, coco_2017_val_100_panoptic, lvis_v1_train, lvis_v1_val, lvis_v1_test_dev, lvis_v1_test_challenge, lvis_v0.5_train, lvis_v0.5_val, lvis_v0.5_val_rand_100, lvis_v0.5_test, lvis_v0.5_train_cocofied, lvis_v0.5_val_cocofied, cityscapes_fine_instance_seg_train, cityscapes_fine_sem_seg_train, cityscapes_fine_instance_seg_val, cityscapes_fine_sem_seg_val, cityscapes_fine_instance_seg_test, cityscapes_fine_sem_seg_test, cityscapes_fine_panoptic_train, cityscapes_fine_panoptic_val, voc_2007_trainval, voc_2007_train, voc_2007_val, voc_2007_test, voc_2012_trainval, voc_2012_train, voc_2012_val, ade20k_sem_seg_train, ade20k_sem_seg_val\""
     ]
    }
   ],
   "source": [
    "dataloader = build_detection_test_loader(cfg, 'DeepFruits_train')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'AspectRatioGroupedDataset' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[44], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mdataloader\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mTypeError\u001B[0m: object of type 'AspectRatioGroupedDataset' has no len()"
     ]
    }
   ],
   "source": [
    "len(dataloader)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cose-ia/COCO_dataset_transformations/super_pycocotools/detectron.py:55: UserWarning: Dataset 'DeepFruits_train' is already registered!\n",
      "  warnings.warn(str(e))\n",
      "/home/cose-ia/COCO_dataset_transformations/super_pycocotools/detectron.py:55: UserWarning: Dataset 'SIXray_train' is already registered!\n",
      "  warnings.warn(str(e))\n"
     ]
    },
    {
     "data": {
      "text/plain": "[('DeepFruits_train',),\n ('SIXray_train', 'SIXray_test'),\n ('clipart_train', 'clipart_test'),\n ('comic_train', 'comic_test'),\n ('watercolor_train', 'watercolor_test'),\n ('CrowdHuman_train', 'CrowdHuman_val'),\n ('DIOR_train', 'DIOR_val', 'DIOR_test'),\n ('DOTA_train', 'DOTA_val', 'DOTA_test'),\n ('fashionpedia_train', 'fashionpedia_val'),\n ('iWildCam_train', 'iWildCam_test'),\n ('KITTI_train', 'KITTI_val'),\n ('LogoDet-3K_train',),\n ('Oktoberfest_train',),\n ('VisDrone_train', 'VisDrone_val', 'VisDrone_test')]"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from super_pycocotools.detectron import register\n",
    "\n",
    "register(super_ann_file)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "cfg.DATASETS.TRAIN = 'DeepFruits_train'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "cfg.SOLVER.IMS_PER_BATCH = 1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "dataloader = build_detection_train_loader(cfg)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [],
   "source": [
    "dataloader = build_detection_test_loader(cfg, 'DeepFruits_train')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "data": {
      "text/plain": "dict_keys(['file_name', 'height', 'width', 'image_id', 'image', 'instances'])"
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = next(iter(dataloader))\n",
    "data[0][\"image_id\"]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "data": {
      "text/plain": "{'file_name': '/home/cose-ia/data/deepFruits/train2017/n12710693_9822.png',\n 'height': 375,\n 'width': 500,\n 'image_id': 459,\n 'image': tensor([[[200, 198, 195,  ..., 214, 213, 213],\n          [198, 196, 194,  ..., 215, 214, 214],\n          [193, 192, 192,  ..., 218, 218, 218],\n          ...,\n          [  9,   9,   9,  ..., 116, 116, 117],\n          [  8,   9,  10,  ..., 100,  98,  97],\n          [  8,   9,  11,  ...,  93,  90,  89]],\n \n         [[160, 159, 155,  ..., 167, 165, 164],\n          [159, 159, 155,  ..., 169, 167, 166],\n          [158, 158, 156,  ..., 173, 172, 171],\n          ...,\n          [ 89,  89,  90,  ..., 199, 200, 200],\n          [ 88,  89,  91,  ..., 185, 184, 183],\n          [ 88,  89,  91,  ..., 179, 177, 176]],\n \n         [[ 64,  63,  62,  ...,  83,  79,  78],\n          [ 64,  63,  63,  ...,  84,  81,  80],\n          [ 63,  63,  64,  ...,  87,  84,  83],\n          ...,\n          [ 58,  58,  58,  ..., 181, 181, 182],\n          [ 57,  58,  59,  ..., 167, 164, 164],\n          [ 57,  58,  59,  ..., 161, 157, 156]]], dtype=torch.uint8),\n 'instances': Instances(num_instances=9, image_height=800, image_width=1067, fields=[gt_boxes: Boxes(tensor([[496., 189., 764., 482.],\n         [231., 170., 502., 454.],\n         [139., 136., 338., 324.],\n         [ 79., 475., 423., 768.],\n         [368., 388., 651., 640.],\n         [643., 320., 858., 567.],\n         [762., 571., 910., 723.],\n         [549., 590., 698., 714.],\n         [677., 541., 792., 652.]])), gt_classes: tensor([6, 6, 6, 6, 6, 6, 6, 6, 6])])}"
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n"
     ]
    }
   ],
   "source": [
    "image_ids = []\n",
    "iteration = iter(dataloader)\n",
    "for i in range(457):\n",
    "    datas = next(iteration)\n",
    "    if i % 100 == 0:\n",
    "        print(i)\n",
    "    for data in datas:\n",
    "        image_ids.append(data[\"image_id\"])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "[1,\n 2,\n 3,\n 4,\n 5,\n 6,\n 7,\n 8,\n 9,\n 10,\n 11,\n 12,\n 13,\n 14,\n 15,\n 16,\n 17,\n 18,\n 19,\n 20,\n 21,\n 22,\n 23,\n 24,\n 25,\n 26,\n 27,\n 28,\n 29,\n 30,\n 31,\n 32,\n 33,\n 34,\n 35,\n 36,\n 37,\n 38,\n 39,\n 40,\n 41,\n 42,\n 43,\n 44,\n 45,\n 46,\n 47,\n 48,\n 49,\n 50,\n 51,\n 52,\n 53,\n 54,\n 55,\n 56,\n 57,\n 58,\n 59,\n 60,\n 61,\n 62,\n 63,\n 64,\n 65,\n 66,\n 67,\n 68,\n 69,\n 70,\n 71,\n 72,\n 73,\n 74,\n 75,\n 76,\n 77,\n 78,\n 79,\n 80,\n 81,\n 82,\n 84,\n 85,\n 86,\n 87,\n 88,\n 89,\n 90,\n 91,\n 92,\n 93,\n 94,\n 95,\n 96,\n 97,\n 98,\n 99,\n 100,\n 101,\n 102,\n 103,\n 104,\n 105,\n 106,\n 107,\n 108,\n 109,\n 110,\n 111,\n 112,\n 113,\n 114,\n 115,\n 116,\n 117,\n 118,\n 119,\n 120,\n 121,\n 122,\n 123,\n 124,\n 125,\n 126,\n 127,\n 128,\n 129,\n 130,\n 131,\n 132,\n 133,\n 134,\n 135,\n 136,\n 137,\n 138,\n 139,\n 140,\n 141,\n 142,\n 143,\n 144,\n 145,\n 146,\n 147,\n 148,\n 149,\n 150,\n 151,\n 152,\n 153,\n 154,\n 155,\n 156,\n 157,\n 158,\n 159,\n 160,\n 161,\n 162,\n 163,\n 164,\n 165,\n 166,\n 167,\n 168,\n 169,\n 170,\n 171,\n 172,\n 173,\n 174,\n 175,\n 176,\n 177,\n 178,\n 179,\n 180,\n 181,\n 182,\n 183,\n 184,\n 185,\n 186,\n 187,\n 188,\n 189,\n 190,\n 191,\n 192,\n 193,\n 194,\n 195,\n 196,\n 197,\n 198,\n 199,\n 200,\n 201,\n 202,\n 203,\n 204,\n 205,\n 206,\n 207,\n 208,\n 209,\n 210,\n 211,\n 212,\n 213,\n 214,\n 215,\n 216,\n 217,\n 218,\n 219,\n 220,\n 221,\n 222,\n 223,\n 224,\n 225,\n 226,\n 227,\n 228,\n 229,\n 230,\n 231,\n 232,\n 233,\n 234,\n 235,\n 236,\n 237,\n 238,\n 239,\n 240,\n 241,\n 242,\n 243,\n 244,\n 245,\n 246,\n 247,\n 248,\n 249,\n 250,\n 251,\n 252,\n 253,\n 254,\n 255,\n 256,\n 257,\n 258,\n 259,\n 260,\n 261,\n 262,\n 263,\n 264,\n 265,\n 266,\n 267,\n 268,\n 269,\n 270,\n 271,\n 272,\n 273,\n 274,\n 275,\n 276,\n 277,\n 278,\n 279,\n 280,\n 281,\n 282,\n 283,\n 284,\n 285,\n 286,\n 287,\n 288,\n 289,\n 290,\n 291,\n 292,\n 293,\n 294,\n 295,\n 296,\n 297,\n 298,\n 299,\n 300,\n 301,\n 302,\n 303,\n 304,\n 305,\n 306,\n 307,\n 308,\n 309,\n 310,\n 311,\n 312,\n 313,\n 314,\n 315,\n 316,\n 317,\n 318,\n 319,\n 320,\n 321,\n 322,\n 323,\n 324,\n 325,\n 326,\n 327,\n 328,\n 329,\n 330,\n 331,\n 332,\n 333,\n 334,\n 335,\n 336,\n 337,\n 338,\n 339,\n 340,\n 341,\n 342,\n 343,\n 344,\n 345,\n 346,\n 347,\n 348,\n 349,\n 350,\n 351,\n 352,\n 353,\n 354,\n 355,\n 356,\n 357,\n 358,\n 359,\n 360,\n 361,\n 362,\n 364,\n 365,\n 366,\n 367,\n 368,\n 369,\n 370,\n 371,\n 372,\n 373,\n 374,\n 375,\n 376,\n 377,\n 378,\n 379,\n 380,\n 381,\n 382,\n 383,\n 384,\n 385,\n 386,\n 387,\n 388,\n 389,\n 390,\n 391,\n 392,\n 393,\n 394,\n 395,\n 396,\n 397,\n 398,\n 399,\n 400,\n 401,\n 402,\n 403,\n 404,\n 405,\n 406,\n 407,\n 408,\n 409,\n 410,\n 411,\n 412,\n 413,\n 414,\n 415,\n 416,\n 417,\n 418,\n 419,\n 420,\n 421,\n 422,\n 423,\n 424,\n 425,\n 426,\n 427,\n 428,\n 429,\n 430,\n 431,\n 432,\n 433,\n 434,\n 435,\n 436,\n 437,\n 438,\n 439,\n 440,\n 441,\n 442,\n 443,\n 444,\n 445,\n 446,\n 447,\n 448,\n 449,\n 450,\n 451,\n 452,\n 453,\n 454,\n 455,\n 456,\n 457,\n 458,\n 459]"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(image_ids)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "data": {
      "text/plain": "457"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(set(image_ids)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "outputs": [
    {
     "data": {
      "text/plain": "30"
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(set(image_ids[-30:])))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "resnet = ResNet(cfg).cuda()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [],
   "source": [
    "data = next(iter(dataloader))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "for data in dataloader:\n",
    "    outputs = resnet(data)\n",
    "    break"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([2, 1024])"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "from detectron2.modeling import build_backbone"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "backbone = build_backbone(cfg)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "ResNet(\n  (stem): BasicStem(\n    (conv1): Conv2d(\n      3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n      (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n    )\n  )\n  (res2): Sequential(\n    (0): BottleneckBlock(\n      (shortcut): Conv2d(\n        64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n        (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n      )\n      (conv1): Conv2d(\n        64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n        (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n      )\n      (conv2): Conv2d(\n        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n        (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n      )\n      (conv3): Conv2d(\n        64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n        (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n      )\n    )\n    (1): BottleneckBlock(\n      (conv1): Conv2d(\n        256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n        (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n      )\n      (conv2): Conv2d(\n        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n        (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n      )\n      (conv3): Conv2d(\n        64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n        (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n      )\n    )\n    (2): BottleneckBlock(\n      (conv1): Conv2d(\n        256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n        (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n      )\n      (conv2): Conv2d(\n        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n        (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n      )\n      (conv3): Conv2d(\n        64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n        (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n      )\n    )\n  )\n  (res3): Sequential(\n    (0): BottleneckBlock(\n      (shortcut): Conv2d(\n        256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n        (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n      )\n      (conv1): Conv2d(\n        256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False\n        (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n      )\n      (conv2): Conv2d(\n        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n        (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n      )\n      (conv3): Conv2d(\n        128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n        (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n      )\n    )\n    (1): BottleneckBlock(\n      (conv1): Conv2d(\n        512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n        (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n      )\n      (conv2): Conv2d(\n        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n        (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n      )\n      (conv3): Conv2d(\n        128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n        (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n      )\n    )\n    (2): BottleneckBlock(\n      (conv1): Conv2d(\n        512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n        (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n      )\n      (conv2): Conv2d(\n        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n        (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n      )\n      (conv3): Conv2d(\n        128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n        (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n      )\n    )\n    (3): BottleneckBlock(\n      (conv1): Conv2d(\n        512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n        (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n      )\n      (conv2): Conv2d(\n        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n        (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n      )\n      (conv3): Conv2d(\n        128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n        (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n      )\n    )\n  )\n  (res4): Sequential(\n    (0): BottleneckBlock(\n      (shortcut): Conv2d(\n        512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n        (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n      )\n      (conv1): Conv2d(\n        512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False\n        (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n      )\n      (conv2): Conv2d(\n        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n        (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n      )\n      (conv3): Conv2d(\n        256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n        (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n      )\n    )\n    (1): BottleneckBlock(\n      (conv1): Conv2d(\n        1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n        (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n      )\n      (conv2): Conv2d(\n        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n        (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n      )\n      (conv3): Conv2d(\n        256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n        (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n      )\n    )\n    (2): BottleneckBlock(\n      (conv1): Conv2d(\n        1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n        (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n      )\n      (conv2): Conv2d(\n        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n        (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n      )\n      (conv3): Conv2d(\n        256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n        (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n      )\n    )\n    (3): BottleneckBlock(\n      (conv1): Conv2d(\n        1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n        (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n      )\n      (conv2): Conv2d(\n        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n        (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n      )\n      (conv3): Conv2d(\n        256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n        (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n      )\n    )\n    (4): BottleneckBlock(\n      (conv1): Conv2d(\n        1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n        (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n      )\n      (conv2): Conv2d(\n        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n        (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n      )\n      (conv3): Conv2d(\n        256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n        (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n      )\n    )\n    (5): BottleneckBlock(\n      (conv1): Conv2d(\n        1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n        (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n      )\n      (conv2): Conv2d(\n        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n        (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n      )\n      (conv3): Conv2d(\n        256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n        (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n      )\n    )\n  )\n)"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "backbone.to('cuda')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "images = resnet.preprocess_image(data)\n",
    "features = backbone(images.tensor)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "['res4']"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(features)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "avgpool = nn.AdaptiveAvgPool2d(output_size=(1, 1))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "CfgNode({'NAME': 'RPN', 'MIN_SIZE': 0})"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg.MODEL.PROPOSAL_GENERATOR"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "features = [features[f] for f in features.keys()]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([2, 1024, 50, 68])"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features[0].shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([2, 1024, 50, 68])"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.cat(features).shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "data": {
      "text/plain": "CfgNode({'DEPTH': 50, 'OUT_FEATURES': ['res4'], 'NUM_GROUPS': 1, 'NORM': 'FrozenBN', 'WIDTH_PER_GROUP': 64, 'STRIDE_IN_1X1': True, 'RES5_DILATION': 1, 'RES2_OUT_CHANNELS': 256, 'STEM_OUT_CHANNELS': 64, 'DEFORM_ON_PER_STAGE': [False, False, False, False], 'DEFORM_MODULATED': False, 'DEFORM_NUM_GROUPS': 1})"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg.MODEL.RESNETS"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n",
      "0\n",
      "{}\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "output1 = resnet(data)\n",
    "output2 = resnet(data)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "from detectron2.checkpoint import DetectionCheckpointer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "cfg.MODEL.WEIGHTS = \"https://dl.fbaipublicfiles.com/detectron2/COCO-Detection/faster_rcnn_R_50_C4_1x/137257644/model_final_721ade.pkl\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The checkpoint state_dict contains keys that are not used by the model:\n",
      "  \u001B[35mproposal_generator.rpn_head.conv.{bias, weight}\u001B[0m\n",
      "  \u001B[35mproposal_generator.rpn_head.objectness_logits.{bias, weight}\u001B[0m\n",
      "  \u001B[35mproposal_generator.rpn_head.anchor_deltas.{bias, weight}\u001B[0m\n",
      "  \u001B[35mroi_heads.res5.0.shortcut.weight\u001B[0m\n",
      "  \u001B[35mroi_heads.res5.0.shortcut.norm.{bias, running_mean, running_var, weight}\u001B[0m\n",
      "  \u001B[35mroi_heads.res5.0.conv1.weight\u001B[0m\n",
      "  \u001B[35mroi_heads.res5.0.conv1.norm.{bias, running_mean, running_var, weight}\u001B[0m\n",
      "  \u001B[35mroi_heads.res5.0.conv2.weight\u001B[0m\n",
      "  \u001B[35mroi_heads.res5.0.conv2.norm.{bias, running_mean, running_var, weight}\u001B[0m\n",
      "  \u001B[35mroi_heads.res5.0.conv3.weight\u001B[0m\n",
      "  \u001B[35mroi_heads.res5.0.conv3.norm.{bias, running_mean, running_var, weight}\u001B[0m\n",
      "  \u001B[35mroi_heads.res5.1.conv1.weight\u001B[0m\n",
      "  \u001B[35mroi_heads.res5.1.conv1.norm.{bias, running_mean, running_var, weight}\u001B[0m\n",
      "  \u001B[35mroi_heads.res5.1.conv2.weight\u001B[0m\n",
      "  \u001B[35mroi_heads.res5.1.conv2.norm.{bias, running_mean, running_var, weight}\u001B[0m\n",
      "  \u001B[35mroi_heads.res5.1.conv3.weight\u001B[0m\n",
      "  \u001B[35mroi_heads.res5.1.conv3.norm.{bias, running_mean, running_var, weight}\u001B[0m\n",
      "  \u001B[35mroi_heads.res5.2.conv1.weight\u001B[0m\n",
      "  \u001B[35mroi_heads.res5.2.conv1.norm.{bias, running_mean, running_var, weight}\u001B[0m\n",
      "  \u001B[35mroi_heads.res5.2.conv2.weight\u001B[0m\n",
      "  \u001B[35mroi_heads.res5.2.conv2.norm.{bias, running_mean, running_var, weight}\u001B[0m\n",
      "  \u001B[35mroi_heads.res5.2.conv3.weight\u001B[0m\n",
      "  \u001B[35mroi_heads.res5.2.conv3.norm.{bias, running_mean, running_var, weight}\u001B[0m\n",
      "  \u001B[35mroi_heads.box_predictor.cls_score.{bias, weight}\u001B[0m\n",
      "  \u001B[35mroi_heads.box_predictor.bbox_pred.{bias, weight}\u001B[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": "{'__author__': 'Detectron2 Model Zoo'}"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpointer = DetectionCheckpointer(resnet)\n",
    "checkpointer.load(cfg.MODEL.WEIGHTS)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "output3 = resnet(data)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(True, device='cuda:0')"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(output2 == output1).all()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(False, device='cuda:0')"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(output2 == output3).all()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([1024])"
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat([output3.mean(0).unsqueeze(0)]).mean(0).shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [],
   "source": [
    "outputs = torch.cat([output3, output3])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([2, 1024])"
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [],
   "source": [
    "mean1 = outputs.mean(0).unsqueeze(0)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [],
   "source": [
    "mean2 = (outputs.sum(0) / 2).unsqueeze(0)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(True, device='cuda:0')"
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(mean1 == mean2).all()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('./output.pkl', 'wb') as f:\n",
    "    pickle.dump(outputs, f)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "data": {
      "text/plain": "<function datasets.create_COCO_tree.create_coco_tree(root, directory)>"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets.create_COCO_tree import create_coco_tree\n",
    "\n",
    "create_coco_tree"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "from detectron2.data.build import get_detection_dataset_dicts"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "from super_pycocotools.detectron import register"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cose-ia/COCO_dataset_transformations/super_pycocotools/detectron.py:55: UserWarning: Dataset 'DeepFruits_train' is already registered!\n",
      "  warnings.warn(str(e))\n",
      "/home/cose-ia/COCO_dataset_transformations/super_pycocotools/detectron.py:55: UserWarning: Dataset 'SIXray_train' is already registered!\n",
      "  warnings.warn(str(e))\n"
     ]
    },
    {
     "data": {
      "text/plain": "[('DeepFruits_train',),\n ('SIXray_train', 'SIXray_test'),\n ('clipart_train', 'clipart_test'),\n ('comic_train', 'comic_test'),\n ('watercolor_train', 'watercolor_test'),\n ('CrowdHuman_train', 'CrowdHuman_val'),\n ('DIOR_train', 'DIOR_val', 'DIOR_test'),\n ('DOTA_train', 'DOTA_val', 'DOTA_test'),\n ('fashionpedia_train', 'fashionpedia_val'),\n ('iWildCam_train', 'iWildCam_test'),\n ('KITTI_train', 'KITTI_val'),\n ('LogoDet-3K_train',),\n ('Oktoberfest_train',),\n ('VisDrone_train', 'VisDrone_val', 'VisDrone_test')]"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "register(\"./datasets.json\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "cfg.DATASETS.TRAIN = 'DeepFruits_train'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [],
   "source": [
    "dataset = get_detection_dataset_dicts(\n",
    "    cfg.DATASETS.TRAIN,\n",
    "    filter_empty=cfg.DATALOADER.FILTER_EMPTY_ANNOTATIONS,\n",
    "    min_keypoints=cfg.MODEL.ROI_KEYPOINT_HEAD.MIN_KEYPOINTS_PER_IMAGE\n",
    "    if cfg.MODEL.KEYPOINT_ON\n",
    "    else 0,\n",
    "    proposal_files=cfg.DATASETS.PROPOSAL_FILES_TRAIN if cfg.MODEL.LOAD_PROPOSALS else None,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "import torch.utils.data as torchdata"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "data": {
      "text/plain": "False"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isinstance(dataset, torchdata.IterableDataset)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'class_to_idx'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[23], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdata\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Subset, DataLoader\n\u001B[0;32m----> 3\u001B[0m subsets \u001B[38;5;241m=\u001B[39m {target: Subset(dataset, [i \u001B[38;5;28;01mfor\u001B[39;00m i, (x, y) \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(dataset) \u001B[38;5;28;01mif\u001B[39;00m y \u001B[38;5;241m==\u001B[39m target]) \u001B[38;5;28;01mfor\u001B[39;00m _, target \u001B[38;5;129;01min\u001B[39;00m \u001B[43mdataset\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mclass_to_idx\u001B[49m\u001B[38;5;241m.\u001B[39mitems()}\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'list' object has no attribute 'class_to_idx'"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Subset\n",
    "\n",
    "subsets = {target: Subset(dataset, [i for i, (x, y) in enumerate(dataset) if y == target]) for _, target in\n",
    "           dataset.class_to_idx.items()}\n",
    "# loaders = {target: DataLoader(subset) for target, subset in subsets.items()}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "data": {
      "text/plain": "dict_keys(['file_name', 'height', 'width', 'image_id', 'annotations'])"
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0].keys()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [
    {
     "data": {
      "text/plain": "{'iscrowd': 0,\n 'bbox': [24, 187, 158, 166],\n 'category_id': 0,\n 'bbox_mode': <BoxMode.XYWH_ABS: 1>}"
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]['annotations'][0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [],
   "source": [
    "data = dataset[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [],
   "source": [
    "annotations = data['annotations']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [],
   "source": [
    "data['annotations'] = []"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [],
   "source": [
    "subsets = {}\n",
    "for data in dataset:\n",
    "    append_ids = []\n",
    "    data_wo_ann = data.copy()\n",
    "    data_wo_ann['annotations'] = []\n",
    "    for annotation in data['annotations']:\n",
    "        category_id = annotation['category_id']\n",
    "        if category_id not in subsets:\n",
    "            subsets.update({category_id: [data_wo_ann]})\n",
    "        if category_id not in append_ids:\n",
    "            subsets[category_id].append(data_wo_ann)\n",
    "            append_ids.append(category_id)\n",
    "\n",
    "        subsets[category_id][-1]['annotations'].append(annotation)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [
    {
     "data": {
      "text/plain": "[{'file_name': '/home/cose-ia/data/deepFruits/train2017/1460105322_1756a2ed7f.png',\n  'height': 375,\n  'width': 500,\n  'image_id': 1,\n  'annotations': [{'iscrowd': 0,\n    'bbox': [24, 187, 158, 166],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [125, 117, 164, 204],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [280, 177, 75, 98],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [306, 47, 156, 199],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [445, 47, 52, 96],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [246, 42, 80, 120],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [84, 36, 164, 115],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>}]},\n {'file_name': '/home/cose-ia/data/deepFruits/train2017/1460105322_1756a2ed7f.png',\n  'height': 375,\n  'width': 500,\n  'image_id': 1,\n  'annotations': [{'iscrowd': 0,\n    'bbox': [24, 187, 158, 166],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [125, 117, 164, 204],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [280, 177, 75, 98],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [306, 47, 156, 199],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [445, 47, 52, 96],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [246, 42, 80, 120],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [84, 36, 164, 115],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>}]},\n {'file_name': '/home/cose-ia/data/deepFruits/train2017/2009-09-15_23-09-46_utc-strawberries.png',\n  'height': 330,\n  'width': 720,\n  'image_id': 2,\n  'annotations': [{'iscrowd': 0,\n    'bbox': [7, 100, 87, 114],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [80, 34, 69, 84],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [198, 47, 81, 83],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [302, 87, 83, 72],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [406, 26, 63, 67],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [301, 29, 47, 50],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [416, 213, 107, 56],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [464, 257, 75, 69],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [524, 292, 75, 35],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [575, 250, 116, 74],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>}]},\n {'file_name': '/home/cose-ia/data/deepFruits/train2017/3007.png',\n  'height': 326,\n  'width': 580,\n  'image_id': 3,\n  'annotations': [{'iscrowd': 0,\n    'bbox': [253, 159, 87, 90],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [455, 74, 46, 69],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [488, 106, 29, 45],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [449, 29, 50, 41],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [76, 106, 55, 49],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>}]},\n {'file_name': '/home/cose-ia/data/deepFruits/train2017/6497592197_d4a363c418_b.png',\n  'height': 1000,\n  'width': 667,\n  'image_id': 4,\n  'annotations': [{'iscrowd': 0,\n    'bbox': [130, 535, 158, 193],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [332, 595, 198, 223],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>}]},\n {'file_name': '/home/cose-ia/data/deepFruits/train2017/8.png',\n  'height': 768,\n  'width': 1024,\n  'image_id': 5,\n  'annotations': [{'iscrowd': 0,\n    'bbox': [6, 500, 134, 157],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [107, 582, 157, 132],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [191, 426, 210, 189],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [392, 469, 197, 216],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [592, 467, 170, 154],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [734, 499, 145, 141],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [770, 393, 160, 138],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [729, 247, 155, 148],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [867, 87, 149, 178],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [546, 123, 99, 95],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [474, 65, 167, 98],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>}]},\n {'file_name': '/home/cose-ia/data/deepFruits/train2017/DSC_0041.png',\n  'height': 529,\n  'width': 800,\n  'image_id': 6,\n  'annotations': [{'iscrowd': 0,\n    'bbox': [210, 376, 122, 112],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [198, 298, 111, 91],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [115, 352, 98, 94],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [180, 269, 96, 61],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>}]},\n {'file_name': '/home/cose-ia/data/deepFruits/train2017/Dalat_Strawberry_garden.png',\n  'height': 448,\n  'width': 602,\n  'image_id': 7,\n  'annotations': [{'iscrowd': 0,\n    'bbox': [455, 240, 76, 122],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [547, 218, 36, 39],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [445, 186, 44, 80],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [383, 236, 70, 110],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [397, 321, 75, 90],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [342, 287, 67, 90],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [305, 302, 57, 108],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [272, 291, 42, 95],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [358, 195, 38, 49],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [292, 224, 53, 50],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [254, 155, 41, 40],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [251, 279, 40, 56],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [212, 294, 53, 54],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [213, 79, 28, 41],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [137, 141, 43, 61],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [133, 215, 31, 67],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [144, 239, 30, 48],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [106, 267, 41, 58],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [106, 227, 30, 38],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [81, 233, 33, 45],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [54, 204, 40, 38],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [53, 241, 28, 45],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [30, 247, 27, 27],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [38, 164, 29, 35],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [36, 200, 30, 39],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [18, 169, 22, 31],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [216, 197, 25, 33],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>}]},\n {'file_name': '/home/cose-ia/data/deepFruits/train2017/FARstevenson_6.png',\n  'height': 465,\n  'width': 620,\n  'image_id': 8,\n  'annotations': [{'iscrowd': 0,\n    'bbox': [468, 270, 43, 39],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [497, 262, 55, 66],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [553, 230, 58, 62],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [542, 170, 33, 71],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [543, 120, 67, 55],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [426, 129, 68, 59],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [397, 184, 101, 99],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [434, 276, 66, 65],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [400, 80, 73, 67],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [209, 180, 81, 89],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [167, 197, 63, 73],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [144, 170, 42, 47],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [76, 154, 78, 94],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [36, 155, 54, 55],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [44, 99, 46, 48],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [79, 89, 68, 48],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [153, 105, 46, 67],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [186, 82, 52, 62],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [211, 40, 48, 71],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>}]},\n {'file_name': '/home/cose-ia/data/deepFruits/train2017/IMG_0954.png',\n  'height': 768,\n  'width': 1024,\n  'image_id': 9,\n  'annotations': [{'iscrowd': 0,\n    'bbox': [456, 144, 157, 149],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [565, 214, 129, 147],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [598, 436, 195, 196],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [805, 488, 161, 121],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [826, 594, 187, 135],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [441, 461, 143, 129],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [464, 326, 133, 130],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [304, 472, 160, 122],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [217, 408, 94, 63],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [114, 410, 79, 71],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>}]},\n {'file_name': '/home/cose-ia/data/deepFruits/train2017/IMG_5392.png',\n  'height': 427,\n  'width': 640,\n  'image_id': 10,\n  'annotations': [{'iscrowd': 0,\n    'bbox': [51, 187, 98, 111],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [445, 186, 71, 88],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [386, 38, 58, 77],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>}]},\n {'file_name': '/home/cose-ia/data/deepFruits/train2017/NC-Strawberries.png',\n  'height': 655,\n  'width': 1000,\n  'image_id': 11,\n  'annotations': [{'iscrowd': 0,\n    'bbox': [143, 230, 199, 238],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [283, 377, 153, 199],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [402, 304, 171, 213],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [367, 468, 168, 175],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [840, 528, 152, 123],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [952, 291, 46, 107],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [152, 429, 171, 202],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>}]},\n {'file_name': '/home/cose-ia/data/deepFruits/train2017/PickYourOwnStrawberries.png',\n  'height': 350,\n  'width': 500,\n  'image_id': 12,\n  'annotations': [{'iscrowd': 0,\n    'bbox': [187, 53, 161, 159],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [304, 125, 135, 158],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [435, 181, 62, 112],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [141, 222, 104, 123],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [38, 161, 111, 130],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [137, 99, 60, 87],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [8, 61, 121, 112],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [408, 134, 48, 64],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>}]},\n {'file_name': '/home/cose-ia/data/deepFruits/train2017/SpringStrawberries.png',\n  'height': 426,\n  'width': 640,\n  'image_id': 13,\n  'annotations': [{'iscrowd': 0,\n    'bbox': [214, 278, 118, 143],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [156, 203, 139, 128],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [264, 18, 218, 274],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [91, 55, 189, 183],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [44, 119, 64, 105],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [6, 18, 102, 111],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>}]},\n {'file_name': '/home/cose-ia/data/deepFruits/train2017/Strawberry-Close-up.png',\n  'height': 750,\n  'width': 1000,\n  'image_id': 14,\n  'annotations': [{'iscrowd': 0,\n    'bbox': [637, 326, 242, 301],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [555, 284, 141, 151],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [467, 504, 233, 210],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [346, 260, 178, 182],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [175, 115, 145, 146],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [194, 549, 132, 130],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [19, 423, 105, 107],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [131, 486, 83, 94],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [89, 308, 122, 111],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [55, 554, 87, 100],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [158, 556, 72, 85],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>}]},\n {'file_name': '/home/cose-ia/data/deepFruits/train2017/ar130598387413248.png',\n  'height': 342,\n  'width': 410,\n  'image_id': 15,\n  'annotations': [{'iscrowd': 0,\n    'bbox': [292, 155, 91, 74],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [172, 127, 116, 89],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [140, 218, 103, 108],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [81, 173, 82, 104],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [43, 93, 91, 92],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>}]},\n {'file_name': '/home/cose-ia/data/deepFruits/train2017/berryfieldB1.png',\n  'height': 960,\n  'width': 1280,\n  'image_id': 16,\n  'annotations': [{'iscrowd': 0,\n    'bbox': [273, 243, 237, 193],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [450, 178, 132, 153],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [586, 267, 181, 199],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [704, 333, 229, 237],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [894, 383, 127, 151],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [878, 523, 152, 143],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [749, 530, 145, 154],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [763, 669, 121, 101],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [563, 596, 185, 155],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [509, 427, 166, 203],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [648, 438, 121, 157],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [431, 552, 129, 133],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [281, 478, 168, 170],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>}]},\n {'file_name': '/home/cose-ia/data/deepFruits/train2017/iStock-strawberries-closeup-web.png',\n  'height': 682,\n  'width': 1024,\n  'image_id': 17,\n  'annotations': [{'iscrowd': 0,\n    'bbox': [54, 261, 208, 263],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [541, 184, 288, 193],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [795, 220, 217, 197],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [641, 370, 217, 211],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [638, 65, 260, 125],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>}]},\n {'file_name': '/home/cose-ia/data/deepFruits/train2017/image_0101.png',\n  'height': 960,\n  'width': 1280,\n  'image_id': 18,\n  'annotations': [{'iscrowd': 0,\n    'bbox': [723, 105, 170, 246],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [693, 352, 169, 252],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [556, 330, 152, 251],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [479, 403, 157, 256],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [352, 462, 200, 294],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [224, 424, 149, 187],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [126, 175, 128, 155],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [8, 198, 88, 121],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>}]},\n {'file_name': '/home/cose-ia/data/deepFruits/train2017/img_2304.png',\n  'height': 1000,\n  'width': 750,\n  'image_id': 19,\n  'annotations': [{'iscrowd': 0,\n    'bbox': [11, 385, 171, 244],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [237, 180, 139, 145],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [395, 248, 202, 212],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [316, 396, 197, 163],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [254, 816, 152, 150],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [153, 371, 131, 141],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>}]},\n {'file_name': '/home/cose-ia/data/deepFruits/train2017/img_8094_two.png',\n  'height': 670,\n  'width': 1000,\n  'image_id': 20,\n  'annotations': [{'iscrowd': 0,\n    'bbox': [60, 276, 260, 186],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [294, 287, 213, 261],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [391, 368, 314, 262],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>}]},\n {'file_name': '/home/cose-ia/data/deepFruits/train2017/maxresdefault-1.png',\n  'height': 1080,\n  'width': 1920,\n  'image_id': 21,\n  'annotations': [{'iscrowd': 0,\n    'bbox': [738, 409, 560, 491],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [1321, 633, 304, 327],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [1723, 553, 182, 485],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [472, 544, 233, 179],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [1545, 540, 200, 176],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>}]},\n {'file_name': '/home/cose-ia/data/deepFruits/train2017/memorial-day-and-fifer-orchards-079.png',\n  'height': 750,\n  'width': 1000,\n  'image_id': 22,\n  'annotations': [{'iscrowd': 0,\n    'bbox': [388, 195, 299, 270],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>}]},\n {'file_name': '/home/cose-ia/data/deepFruits/train2017/midorikawa-strawberry-farm_228041.png',\n  'height': 412,\n  'width': 550,\n  'image_id': 23,\n  'annotations': [{'iscrowd': 0,\n    'bbox': [409, 234, 135, 162],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [296, 258, 130, 120],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [248, 134, 150, 112],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [249, 227, 96, 117],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [132, 181, 135, 123],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [119, 172, 73, 79],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [401, 36, 120, 82],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [103, 148, 42, 119],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [30, 114, 104, 86],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [44, 37, 98, 83],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>}]},\n {'file_name': '/home/cose-ia/data/deepFruits/train2017/p1050832-1.png',\n  'height': 1056,\n  'width': 1408,\n  'image_id': 24,\n  'annotations': [{'iscrowd': 0,\n    'bbox': [1168, 722, 192, 190],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [841, 701, 163, 210],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [778, 623, 174, 136],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [976, 682, 104, 119],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [1322, 415, 78, 151],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [327, 595, 140, 158],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [328, 507, 144, 101],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [201, 549, 143, 156],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [156, 460, 116, 134],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [220, 297, 118, 81],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [262, 453, 79, 75],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>}]},\n {'file_name': '/home/cose-ia/data/deepFruits/train2017/strawberries-1.png',\n  'height': 1129,\n  'width': 1701,\n  'image_id': 25,\n  'annotations': [{'iscrowd': 0,\n    'bbox': [1054, 605, 437, 516],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [661, 348, 468, 499],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [449, 749, 349, 369],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [375, 259, 351, 374],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [1525, 126, 171, 301],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [610, 38, 242, 255],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>}]},\n {'file_name': '/home/cose-ia/data/deepFruits/train2017/strawberries-polytunnel.png',\n  'height': 1200,\n  'width': 1600,\n  'image_id': 26,\n  'annotations': [{'iscrowd': 0,\n    'bbox': [316, 519, 210, 275],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [413, 808, 128, 163],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [307, 889, 134, 209],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [156, 899, 165, 160],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [434, 961, 101, 104],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [556, 727, 116, 150],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [390, 340, 82, 140],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [677, 581, 86, 148],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [658, 777, 86, 122],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [831, 658, 103, 136],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [919, 677, 106, 83],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [1016, 671, 90, 116],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [872, 426, 107, 118],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [764, 113, 129, 158],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [811, 240, 67, 92],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [1109, 687, 78, 92],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [497, 670, 94, 226],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>}]},\n {'file_name': '/home/cose-ia/data/deepFruits/train2017/strawberries-rd-1024x768.png',\n  'height': 768,\n  'width': 1024,\n  'image_id': 27,\n  'annotations': [{'iscrowd': 0,\n    'bbox': [76, 390, 296, 265],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [369, 394, 197, 212],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [582, 387, 167, 153],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [648, 507, 237, 224],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [800, 350, 215, 231],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [225, 209, 239, 183],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>}]},\n {'file_name': '/home/cose-ia/data/deepFruits/train2017/strawberries.png',\n  'height': 768,\n  'width': 1024,\n  'image_id': 28,\n  'annotations': [{'iscrowd': 0,\n    'bbox': [79, 443, 200, 189],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [252, 578, 189, 156],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [506, 506, 202, 209],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [683, 505, 193, 174],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [591, 421, 124, 122],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [708, 367, 157, 167],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [817, 340, 115, 133],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [733, 154, 150, 131],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [682, 269, 123, 125],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [805, 289, 121, 75],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [346, 210, 193, 175],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [474, 178, 105, 131],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [284, 117, 141, 162],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [230, 247, 128, 129],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [220, 383, 133, 126],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [64, 224, 184, 177],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [148, 131, 120, 116],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [796, 68, 135, 95],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [387, 458, 112, 97],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [525, 294, 103, 112],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>}]},\n {'file_name': '/home/cose-ia/data/deepFruits/train2017/strawberry-picking-at.png',\n  'height': 450,\n  'width': 450,\n  'image_id': 29,\n  'annotations': [{'iscrowd': 0,\n    'bbox': [215, 318, 79, 114],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [110, 317, 101, 112],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [63, 284, 71, 112],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [139, 284, 53, 58],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>}]},\n {'file_name': '/home/cose-ia/data/deepFruits/train2017/strawberry_-014_01_zoom.png',\n  'height': 675,\n  'width': 900,\n  'image_id': 30,\n  'annotations': [{'iscrowd': 0,\n    'bbox': [114, 125, 395, 349],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [494, 187, 289, 279],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>}]},\n {'file_name': '/home/cose-ia/data/deepFruits/train2017/strawberryplant.png',\n  'height': 375,\n  'width': 500,\n  'image_id': 31,\n  'annotations': [{'iscrowd': 0,\n    'bbox': [24, 192, 153, 162],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [130, 117, 161, 207],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [280, 177, 77, 102],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [303, 34, 158, 212],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [440, 38, 56, 108],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [83, 37, 162, 119],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [242, 42, 83, 120],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>}]},\n {'file_name': '/home/cose-ia/data/deepFruits/train2017/strawberryplants.png',\n  'height': 426,\n  'width': 640,\n  'image_id': 32,\n  'annotations': [{'iscrowd': 0,\n    'bbox': [35, 167, 123, 161],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [334, 116, 178, 120],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [494, 138, 140, 118],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [403, 235, 128, 113],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [391, 28, 184, 87],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>}]},\n {'file_name': '/home/cose-ia/data/deepFruits/train2017/yum.png',\n  'height': 604,\n  'width': 900,\n  'image_id': 33,\n  'annotations': [{'iscrowd': 0,\n    'bbox': [616, 324, 117, 110],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [527, 256, 78, 80],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [402, 272, 97, 87],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [318, 296, 105, 112],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [259, 264, 80, 90],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [265, 163, 71, 108],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>},\n   {'iscrowd': 0,\n    'bbox': [138, 72, 66, 77],\n    'category_id': 0,\n    'bbox_mode': <BoxMode.XYWH_ABS: 1>}]}]"
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subsets[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "data": {
      "text/plain": "'TrainingSampler'"
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg.DATALOADER.SAMPLER_TRAIN"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "_test_loader_from_config() missing 1 required positional argument: 'dataset_name'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[66], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m dataloader \u001B[38;5;241m=\u001B[39m \u001B[43mbuild_detection_test_loader\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcfg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdataset\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msubsets\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/detectron2/detectron2/config/config.py:207\u001B[0m, in \u001B[0;36mconfigurable.<locals>.wrapper.<locals>.wrapped\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    204\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(orig_func)\n\u001B[1;32m    205\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mwrapped\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m    206\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m _called_with_cfg(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m--> 207\u001B[0m         explicit_args \u001B[38;5;241m=\u001B[39m \u001B[43m_get_args_from_config\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfrom_config\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    208\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m orig_func(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mexplicit_args)\n\u001B[1;32m    209\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[0;32m~/detectron2/detectron2/config/config.py:245\u001B[0m, in \u001B[0;36m_get_args_from_config\u001B[0;34m(from_config_func, *args, **kwargs)\u001B[0m\n\u001B[1;32m    243\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m supported_arg_names:\n\u001B[1;32m    244\u001B[0m         extra_kwargs[name] \u001B[38;5;241m=\u001B[39m kwargs\u001B[38;5;241m.\u001B[39mpop(name)\n\u001B[0;32m--> 245\u001B[0m ret \u001B[38;5;241m=\u001B[39m \u001B[43mfrom_config_func\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    246\u001B[0m \u001B[38;5;66;03m# forward the other arguments to __init__\u001B[39;00m\n\u001B[1;32m    247\u001B[0m ret\u001B[38;5;241m.\u001B[39mupdate(extra_kwargs)\n",
      "\u001B[0;31mTypeError\u001B[0m: _test_loader_from_config() missing 1 required positional argument: 'dataset_name'"
     ]
    }
   ],
   "source": [
    "dataloader = build_detection_test_loader(cfg, dataset=subsets[0])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "{\"dataset\": dataset,\n",
    " \"mapper\": mapper,\n",
    " \"num_workers\": cfg.DATALOADER.NUM_WORKERS,\n",
    " \"sampler\": InferenceSampler(len(dataset))\n",
    " if not isinstance(dataset, torchdata.IterableDataset)\n",
    " else None,\n",
    " }"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [],
   "source": [
    "from detectron2.data.samplers import InferenceSampler\n",
    "from detectron2.data import DatasetMapper\n",
    "\n",
    "dataloader = build_detection_test_loader(\n",
    "    dataset=subsets[0],\n",
    "    mapper=DatasetMapper(cfg, False),\n",
    "    num_workers=cfg.DATALOADER.NUM_WORKERS,\n",
    "    sampler=InferenceSampler(len(subsets[0])),\n",
    "    batch_size=cfg.SOLVER.IMS_PER_BATCH\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [],
   "source": [
    "data = next(iter(dataloader))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [
    {
     "data": {
      "text/plain": "16"
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "outputs": [
    {
     "data": {
      "text/plain": "{'file_name': '/home/cose-ia/data/deepFruits/train2017/Gatton1_valid_20141204T1836_frame0015.png',\n 'height': 964,\n 'width': 1296,\n 'image_id': 288,\n 'image': tensor([[[237, 120,  38,  ..., 143, 158, 154],\n          [207,  38,  39,  ...,  50,  46,  45],\n          [105,  38,  38,  ...,  44,  41,  40],\n          ...,\n          [ 11,  50,  52,  ...,  46,  46,  46],\n          [ 11,  50,  52,  ...,  46,  45,  46],\n          [  2,  11,  11,  ...,  10,  10,  10]],\n \n         [[ 97, 120,   8,  ..., 181, 188, 174],\n          [206,  37,  37,  ...,  60,  54,  47],\n          [103,  35,  35,  ...,  50,  44,  42],\n          ...,\n          [ 11,  51,  51,  ...,  39,  38,  39],\n          [ 11,  51,  51,  ...,  39,  37,  38],\n          [  2,  11,  11,  ...,   9,   8,   8]],\n \n         [[116, 182, 162,  ..., 192, 196, 211],\n          [209,  42,  42,  ...,  62,  57,  53],\n          [107,  40,  40,  ...,  53,  48,  48],\n          ...,\n          [ 13,  61,  62,  ...,  49,  49,  53],\n          [ 14,  64,  64,  ...,  49,  48,  53],\n          [  3,  14,  14,  ...,  11,  10,  12]]], dtype=torch.uint8),\n 'instances': Instances(num_instances=3, image_height=800, image_width=1076, fields=[gt_boxes: Boxes(tensor([[ 26., 278., 151., 401.],\n         [194., 345., 312., 456.],\n         [285., 736., 401., 796.]])), gt_classes: tensor([4, 4, 4])])}"
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [],
   "source": [
    "dataloader = build_detection_train_loader(cfg)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[81], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mdata\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mimage\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\n",
      "\u001B[0;31mTypeError\u001B[0m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "data['image']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "outputs": [
    {
     "data": {
      "text/plain": "CfgNode({'NAME': 'Res5ROIHeads', 'NUM_CLASSES': 80, 'IN_FEATURES': ['res4'], 'IOU_THRESHOLDS': [0.5], 'IOU_LABELS': [0, 1], 'BATCH_SIZE_PER_IMAGE': 512, 'POSITIVE_FRACTION': 0.25, 'SCORE_THRESH_TEST': 0.05, 'NMS_THRESH_TEST': 0.5, 'PROPOSAL_APPEND_GT': True})"
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg.MODEL.ROI_HEADS"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "outputs": [],
   "source": [
    "from detectron2.layers import ShapeSpec"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"No object named '' found in 'ROI_BOX_HEAD' registry!\"",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[92], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mdetectron2\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmodeling\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m build_box_head\n\u001B[0;32m----> 3\u001B[0m box_head \u001B[38;5;241m=\u001B[39m \u001B[43mbuild_box_head\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m      4\u001B[0m \u001B[43m            \u001B[49m\u001B[43mcfg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mShapeSpec\u001B[49m\u001B[43m(\u001B[49m\u001B[43mchannels\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m3\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mheight\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m100\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mwidth\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m100\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m      5\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/detectron2/detectron2/modeling/roi_heads/box_head.py:118\u001B[0m, in \u001B[0;36mbuild_box_head\u001B[0;34m(cfg, input_shape)\u001B[0m\n\u001B[1;32m    114\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    115\u001B[0m \u001B[38;5;124;03mBuild a box head defined by `cfg.MODEL.ROI_BOX_HEAD.NAME`.\u001B[39;00m\n\u001B[1;32m    116\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    117\u001B[0m name \u001B[38;5;241m=\u001B[39m cfg\u001B[38;5;241m.\u001B[39mMODEL\u001B[38;5;241m.\u001B[39mROI_BOX_HEAD\u001B[38;5;241m.\u001B[39mNAME\n\u001B[0;32m--> 118\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mROI_BOX_HEAD_REGISTRY\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m(cfg, input_shape)\n",
      "File \u001B[0;32m~/miniconda3/envs/detr/lib/python3.10/site-packages/fvcore/common/registry.py:71\u001B[0m, in \u001B[0;36mRegistry.get\u001B[0;34m(self, name)\u001B[0m\n\u001B[1;32m     69\u001B[0m ret \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_obj_map\u001B[38;5;241m.\u001B[39mget(name)\n\u001B[1;32m     70\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m ret \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m---> 71\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(\n\u001B[1;32m     72\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNo object named \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m found in \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m registry!\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(name, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_name)\n\u001B[1;32m     73\u001B[0m     )\n\u001B[1;32m     74\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m ret\n",
      "\u001B[0;31mKeyError\u001B[0m: \"No object named '' found in 'ROI_BOX_HEAD' registry!\""
     ]
    }
   ],
   "source": [
    "from detectron2.modeling import build_box_head\n",
    "\n",
    "box_head = build_box_head(\n",
    "    cfg, ShapeSpec(channels=3, height=100, width=100)\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "outputs": [
    {
     "data": {
      "text/plain": "CfgNode({'NAME': '', 'BBOX_REG_LOSS_TYPE': 'smooth_l1', 'BBOX_REG_LOSS_WEIGHT': 1.0, 'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0), 'SMOOTH_L1_BETA': 0.0, 'POOLER_RESOLUTION': 14, 'POOLER_SAMPLING_RATIO': 0, 'POOLER_TYPE': 'ROIAlignV2', 'NUM_FC': 0, 'FC_DIM': 1024, 'NUM_CONV': 0, 'CONV_DIM': 256, 'NORM': '', 'CLS_AGNOSTIC_BBOX_REG': False, 'TRAIN_ON_PRED_BOXES': False, 'USE_FED_LOSS': False, 'USE_SIGMOID_CE': False, 'FED_LOSS_FREQ_WEIGHT_POWER': 0.5, 'FED_LOSS_NUM_CLASSES': 50})"
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg.MODEL.ROI_BOX_HEAD"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "from detectron2.utils.registry import Registry"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"No object named 'GeneralizedRCNN' found in 'META_ARCH' registry!\"",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[15], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mRegistry\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mMETA_ARCH\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcfg\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mMODEL\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mMETA_ARCHITECTURE\u001B[49m\u001B[43m)\u001B[49m(cfg)\n",
      "File \u001B[0;32m~/miniconda3/envs/detr/lib/python3.10/site-packages/fvcore/common/registry.py:71\u001B[0m, in \u001B[0;36mRegistry.get\u001B[0;34m(self, name)\u001B[0m\n\u001B[1;32m     69\u001B[0m ret \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_obj_map\u001B[38;5;241m.\u001B[39mget(name)\n\u001B[1;32m     70\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m ret \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m---> 71\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(\n\u001B[1;32m     72\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNo object named \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m found in \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m registry!\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(name, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_name)\n\u001B[1;32m     73\u001B[0m     )\n\u001B[1;32m     74\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m ret\n",
      "\u001B[0;31mKeyError\u001B[0m: \"No object named 'GeneralizedRCNN' found in 'META_ARCH' registry!\""
     ]
    }
   ],
   "source": [
    "Registry(\"META_ARCH\").get(cfg.MODEL.META_ARCHITECTURE)(cfg)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "outputs": [
    {
     "data": {
      "text/plain": "CfgNode({'NAME': 'Res5ROIHeads', 'NUM_CLASSES': 80, 'IN_FEATURES': ['res4'], 'IOU_THRESHOLDS': [0.5], 'IOU_LABELS': [0, 1], 'BATCH_SIZE_PER_IMAGE': 512, 'POSITIVE_FRACTION': 0.25, 'SCORE_THRESH_TEST': 0.05, 'NMS_THRESH_TEST': 0.5, 'PROPOSAL_APPEND_GT': True})"
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg.MODEL.ROI_HEADS."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "ResNet(\n  (backbone): ResNet(\n    (stem): BasicStem(\n      (conv1): Conv2d(\n        3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n        (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n      )\n    )\n    (res2): Sequential(\n      (0): BottleneckBlock(\n        (shortcut): Conv2d(\n          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n        )\n        (conv1): Conv2d(\n          64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n        )\n        (conv2): Conv2d(\n          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n        )\n        (conv3): Conv2d(\n          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n        )\n      )\n      (1): BottleneckBlock(\n        (conv1): Conv2d(\n          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n        )\n        (conv2): Conv2d(\n          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n        )\n        (conv3): Conv2d(\n          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n        )\n      )\n      (2): BottleneckBlock(\n        (conv1): Conv2d(\n          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n        )\n        (conv2): Conv2d(\n          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n        )\n        (conv3): Conv2d(\n          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n        )\n      )\n    )\n    (res3): Sequential(\n      (0): BottleneckBlock(\n        (shortcut): Conv2d(\n          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n        )\n        (conv1): Conv2d(\n          256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False\n          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n        )\n        (conv2): Conv2d(\n          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n        )\n        (conv3): Conv2d(\n          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n        )\n      )\n      (1): BottleneckBlock(\n        (conv1): Conv2d(\n          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n        )\n        (conv2): Conv2d(\n          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n        )\n        (conv3): Conv2d(\n          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n        )\n      )\n      (2): BottleneckBlock(\n        (conv1): Conv2d(\n          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n        )\n        (conv2): Conv2d(\n          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n        )\n        (conv3): Conv2d(\n          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n        )\n      )\n      (3): BottleneckBlock(\n        (conv1): Conv2d(\n          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n        )\n        (conv2): Conv2d(\n          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n        )\n        (conv3): Conv2d(\n          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n        )\n      )\n    )\n    (res4): Sequential(\n      (0): BottleneckBlock(\n        (shortcut): Conv2d(\n          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n        )\n        (conv1): Conv2d(\n          512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False\n          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n        )\n        (conv2): Conv2d(\n          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n        )\n        (conv3): Conv2d(\n          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n        )\n      )\n      (1): BottleneckBlock(\n        (conv1): Conv2d(\n          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n        )\n        (conv2): Conv2d(\n          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n        )\n        (conv3): Conv2d(\n          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n        )\n      )\n      (2): BottleneckBlock(\n        (conv1): Conv2d(\n          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n        )\n        (conv2): Conv2d(\n          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n        )\n        (conv3): Conv2d(\n          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n        )\n      )\n      (3): BottleneckBlock(\n        (conv1): Conv2d(\n          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n        )\n        (conv2): Conv2d(\n          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n        )\n        (conv3): Conv2d(\n          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n        )\n      )\n      (4): BottleneckBlock(\n        (conv1): Conv2d(\n          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n        )\n        (conv2): Conv2d(\n          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n        )\n        (conv3): Conv2d(\n          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n        )\n      )\n      (5): BottleneckBlock(\n        (conv1): Conv2d(\n          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n        )\n        (conv2): Conv2d(\n          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n        )\n        (conv3): Conv2d(\n          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n        )\n      )\n    )\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n)"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "dict_keys(['LOAD_PROPOSALS', 'MASK_ON', 'KEYPOINT_ON', 'DEVICE', 'META_ARCHITECTURE', 'WEIGHTS', 'PIXEL_MEAN', 'PIXEL_STD', 'BACKBONE', 'FPN', 'PROPOSAL_GENERATOR', 'ANCHOR_GENERATOR', 'RPN', 'ROI_HEADS', 'ROI_BOX_HEAD', 'ROI_BOX_CASCADE_HEAD', 'ROI_MASK_HEAD', 'ROI_KEYPOINT_HEAD', 'SEM_SEG_HEAD', 'PANOPTIC_FPN', 'RETINANET', 'RESNETS'])"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg.MODEL.keys()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "data": {
      "text/plain": "['res4']"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg.MODEL.ROI_HEADS.IN_FEATURES"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "from super_pycocotools.detectron import register"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "datasets = register(super_ann_file)\n",
    "batch_size = 1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "'DeepFruits_train'"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = datasets[0][0]\n",
    "dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "cfg = get_cfg()\n",
    "cfg.DATASETS.TRAIN = dataset\n",
    "cfg.SOLVER.IMS_PER_BATCH = batch_size"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "from roi_pool import ResNetROIPool"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "dataloader = build_detection_train_loader(cfg)\n",
    "pool = ResNetROIPool(cfg).cuda()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "from detectron2.checkpoint import DetectionCheckpointer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The checkpoint state_dict contains keys that are not used by the model:\n",
      "  \u001B[35mproposal_generator.rpn_head.conv.{bias, weight}\u001B[0m\n",
      "  \u001B[35mproposal_generator.rpn_head.objectness_logits.{bias, weight}\u001B[0m\n",
      "  \u001B[35mproposal_generator.rpn_head.anchor_deltas.{bias, weight}\u001B[0m\n",
      "  \u001B[35mroi_heads.res5.0.shortcut.weight\u001B[0m\n",
      "  \u001B[35mroi_heads.res5.0.shortcut.norm.{bias, running_mean, running_var, weight}\u001B[0m\n",
      "  \u001B[35mroi_heads.res5.0.conv1.weight\u001B[0m\n",
      "  \u001B[35mroi_heads.res5.0.conv1.norm.{bias, running_mean, running_var, weight}\u001B[0m\n",
      "  \u001B[35mroi_heads.res5.0.conv2.weight\u001B[0m\n",
      "  \u001B[35mroi_heads.res5.0.conv2.norm.{bias, running_mean, running_var, weight}\u001B[0m\n",
      "  \u001B[35mroi_heads.res5.0.conv3.weight\u001B[0m\n",
      "  \u001B[35mroi_heads.res5.0.conv3.norm.{bias, running_mean, running_var, weight}\u001B[0m\n",
      "  \u001B[35mroi_heads.res5.1.conv1.weight\u001B[0m\n",
      "  \u001B[35mroi_heads.res5.1.conv1.norm.{bias, running_mean, running_var, weight}\u001B[0m\n",
      "  \u001B[35mroi_heads.res5.1.conv2.weight\u001B[0m\n",
      "  \u001B[35mroi_heads.res5.1.conv2.norm.{bias, running_mean, running_var, weight}\u001B[0m\n",
      "  \u001B[35mroi_heads.res5.1.conv3.weight\u001B[0m\n",
      "  \u001B[35mroi_heads.res5.1.conv3.norm.{bias, running_mean, running_var, weight}\u001B[0m\n",
      "  \u001B[35mroi_heads.res5.2.conv1.weight\u001B[0m\n",
      "  \u001B[35mroi_heads.res5.2.conv1.norm.{bias, running_mean, running_var, weight}\u001B[0m\n",
      "  \u001B[35mroi_heads.res5.2.conv2.weight\u001B[0m\n",
      "  \u001B[35mroi_heads.res5.2.conv2.norm.{bias, running_mean, running_var, weight}\u001B[0m\n",
      "  \u001B[35mroi_heads.res5.2.conv3.weight\u001B[0m\n",
      "  \u001B[35mroi_heads.res5.2.conv3.norm.{bias, running_mean, running_var, weight}\u001B[0m\n",
      "  \u001B[35mroi_heads.box_predictor.cls_score.{bias, weight}\u001B[0m\n",
      "  \u001B[35mroi_heads.box_predictor.bbox_pred.{bias, weight}\u001B[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": "{'__author__': 'Detectron2 Model Zoo'}"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_size = len(dataloader.dataset.dataset.dataset)\n",
    "\n",
    "cfg.MODEL.WEIGHTS = \"./model_final_721ade.pkl\"\n",
    "checkpointer = DetectionCheckpointer(pool)\n",
    "checkpointer.load(cfg.MODEL.WEIGHTS)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "import torch"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "class_mean = {}\n",
    "with torch.no_grad():\n",
    "    dataloader_iteration = iter(dataloader)\n",
    "    for i in range(dataset_size):\n",
    "        data = next(dataloader_iteration)\n",
    "        box_features, target_classes = pool(data)\n",
    "\n",
    "        for target_classe in target_classes:\n",
    "            for class_id, box in zip(target_classe, box_features):\n",
    "                if class_id.item() not in class_mean:\n",
    "                    class_mean[class_id.item()] = []\n",
    "                class_mean[class_id.item()].append(box.unsqueeze(dim=0))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "cat = torch.cat(class_mean[0])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(-4.0483, device='cuda:0')"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat.min()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[22], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m get_ipython()\u001B[38;5;241m.\u001B[39mrun_line_magic(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmatplotlib\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mqt\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m----> 2\u001B[0m \u001B[43mplt\u001B[49m\u001B[38;5;241m.\u001B[39mimshow(cat\u001B[38;5;241m.\u001B[39mt()\u001B[38;5;241m.\u001B[39mcpu()\u001B[38;5;241m.\u001B[39mnumpy())\n",
      "\u001B[0;31mNameError\u001B[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "%matplotlib qt\n",
    "plt.imshow(cat.t().cpu().numpy())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "cov = torch.cov(cat.t())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import numpy as np"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([1024, 1024])"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cov.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "45"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.isclose(np.linalg.eig(cov.cpu().numpy())[0], np.zeros(cov.shape[0])).sum()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([271, 1024])"
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "data": {
      "text/plain": "ResNetROIPool(\n  (backbone): ResNet(\n    (stem): BasicStem(\n      (conv1): Conv2d(\n        3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n        (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n      )\n    )\n    (res2): Sequential(\n      (0): BottleneckBlock(\n        (shortcut): Conv2d(\n          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n        )\n        (conv1): Conv2d(\n          64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n        )\n        (conv2): Conv2d(\n          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n        )\n        (conv3): Conv2d(\n          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n        )\n      )\n      (1): BottleneckBlock(\n        (conv1): Conv2d(\n          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n        )\n        (conv2): Conv2d(\n          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n        )\n        (conv3): Conv2d(\n          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n        )\n      )\n      (2): BottleneckBlock(\n        (conv1): Conv2d(\n          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n        )\n        (conv2): Conv2d(\n          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n        )\n        (conv3): Conv2d(\n          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n        )\n      )\n    )\n    (res3): Sequential(\n      (0): BottleneckBlock(\n        (shortcut): Conv2d(\n          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n        )\n        (conv1): Conv2d(\n          256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False\n          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n        )\n        (conv2): Conv2d(\n          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n        )\n        (conv3): Conv2d(\n          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n        )\n      )\n      (1): BottleneckBlock(\n        (conv1): Conv2d(\n          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n        )\n        (conv2): Conv2d(\n          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n        )\n        (conv3): Conv2d(\n          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n        )\n      )\n      (2): BottleneckBlock(\n        (conv1): Conv2d(\n          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n        )\n        (conv2): Conv2d(\n          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n        )\n        (conv3): Conv2d(\n          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n        )\n      )\n      (3): BottleneckBlock(\n        (conv1): Conv2d(\n          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n        )\n        (conv2): Conv2d(\n          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n        )\n        (conv3): Conv2d(\n          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n        )\n      )\n    )\n    (res4): Sequential(\n      (0): BottleneckBlock(\n        (shortcut): Conv2d(\n          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n        )\n        (conv1): Conv2d(\n          512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False\n          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n        )\n        (conv2): Conv2d(\n          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n        )\n        (conv3): Conv2d(\n          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n        )\n      )\n      (1): BottleneckBlock(\n        (conv1): Conv2d(\n          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n        )\n        (conv2): Conv2d(\n          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n        )\n        (conv3): Conv2d(\n          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n        )\n      )\n      (2): BottleneckBlock(\n        (conv1): Conv2d(\n          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n        )\n        (conv2): Conv2d(\n          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n        )\n        (conv3): Conv2d(\n          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n        )\n      )\n      (3): BottleneckBlock(\n        (conv1): Conv2d(\n          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n        )\n        (conv2): Conv2d(\n          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n        )\n        (conv3): Conv2d(\n          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n        )\n      )\n      (4): BottleneckBlock(\n        (conv1): Conv2d(\n          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n        )\n        (conv2): Conv2d(\n          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n        )\n        (conv3): Conv2d(\n          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n        )\n      )\n      (5): BottleneckBlock(\n        (conv1): Conv2d(\n          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n        )\n        (conv2): Conv2d(\n          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n        )\n        (conv3): Conv2d(\n          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n        )\n      )\n    )\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n  (pooler): ROIPooler(\n    (level_poolers): ModuleList(\n      (0): ROIAlign(output_size=(14, 14), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n    )\n  )\n)"
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pool"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "data": {
      "text/plain": "BottleneckBlock(\n  (conv1): Conv2d(\n    1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n    (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n  )\n  (conv2): Conv2d(\n    256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n    (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n  )\n  (conv3): Conv2d(\n    256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n    (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n  )\n)"
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pool.backbone.res4[5]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [],
   "source": [
    "cfg.MODEL.BACKBONE.NAME = 'build_resnet_backbone_with_out_relu'"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}